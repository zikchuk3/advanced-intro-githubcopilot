# How Did We Get Generative AI

Generative AI represents decades of research dating back to the 1960s, evolving from simple typewritten chatbots that relied on keyword-triggered responses from expert knowledge bases. These early systems quickly proved inadequate due to their limited scalability and inability to handle complex language understanding.

### A statistical approach to AI: Machine Learning
The 1990s marked a crucial turning point with the introduction of machine learning algorithms that used statistical approaches to text analysis. Unlike earlier rule-based systems, these algorithms could learn patterns from data without explicit programming, enabling machines to simulate human language understanding by training on text-label pairs to classify and interpret user intentions.

### Neural networks and modern virtual assistants
Neural Networks Era: Advances in computing hardware enabled the development of neural networks and deep learning algorithms, particularly Recurrent Neural Networks (RNNs), which significantly improved natural language processing by better capturing word context within sentences. This technology powered the virtual assistants of the early 2000s, which could interpret human language, identify needs, and execute appropriate actions.

### Present day, Generative AI
After decades of research in the AI field, a new model architecture – called Transformer – overcame the limits of RNNs, being able to get much longer sequences of text as input. Transformers are based on the attention mechanism, enabling the model to give different weights to the inputs it receives, ‘paying more attention’ where the most relevant information is concentrated, regardless of their order in the text sequence.

Most of the recent generative AI models – also known as Large Language Models (LLMs), since they work with textual inputs and outputs – are indeed based on this architecture. What’s interesting about these models – trained on a huge amount of unlabeled data from diverse sources like books, articles and websites – is that they can be adapted to a wide variety of tasks and generate grammatically correct text with a semblance of creativity. So, not only did they incredibly enhance the capacity of a machine to ‘understand’ an input text, but they enabled their capacity to generate an original response in human language.

![Generative AI History](../media/AI-diagram.png)

Now let's look in depth at [Large Language Models](../lesson-1.2/use-llm.md).